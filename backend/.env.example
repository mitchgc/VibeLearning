# Ollama Configuration
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5-coder:7b

# Alternative models you can use:
# OLLAMA_MODEL=gemma2:2b          # Faster, smaller model  
# OLLAMA_MODEL=gpt-oss:20b        # Large general purpose model
# OLLAMA_MODEL=qwen3-coder:30b    # Most capable but requires more resources
# OLLAMA_MODEL=llama3.2:latest    # Good general purpose